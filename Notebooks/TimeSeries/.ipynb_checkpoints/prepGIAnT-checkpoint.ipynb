{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Unwrapped, Geocoded Interferograms from GRFN for GIAnT Analysis\n",
    "\n",
    "In this notebook, we will walk through the various steps involved in importing unwrapped, geocoded interferograms into GIAnT. We will use a Sentinel-1 dataset over Hawaii from the GRFN archive, processed using the ISCE software for this example. \n",
    "\n",
    "We will prepare our data assuming the following\n",
    "\n",
    "1. The geocoded products are not necessarily aligned. We align products on the fly using **GDAL**.\n",
    "\n",
    "2. The regions of interest and reference point will be provided in map coordinates, allowing us to reuse the data preparation scripts across data from different sources.\n",
    "\n",
    "But first, we will spend some time exploring the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Exploring available data\n",
    "\n",
    "Let's take a look at the data for the exercise first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " Let's first change our current working directory to the folder where the data for the tutorial is staged. In my case this is ~/data/giant/kilaeau/2018-unavco-workshop-insar-course. Please use appropriate folder name on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cd /Users/agram/tools/notebooks/TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's list the contents of the staged directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's explore the folder named **download**. This contains GRFN data downloaded from the ASF DAAC using the download-all-2018-06-15.py python script. The python script was automatically generated by ASF's VERTEX interface. We ran this **download** python script in the download folder to fetch the identified datasets from ASF.\n",
    "\n",
    "For your convenience, we have added a section to describe the process we used to generate the python script. However, this will not be the focus of the tutorial. We encourage you to take a look at this section at your own convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Detour: Search process on Vertex for generating download script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following screen shots show the procedure for searching and setting up the download script on ASF's VERTEX interface. The method presented here works on the current interface. Better search and filtering features are expected to be available in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Step 1: Set up search area and date ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Search Page Screenshot](notebook_images/searchASF.png \"ASF Vertex Search Page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Step 2: Search and subset results using text search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ASF Vertex search results show Sentinel-1 SAFE granules as well as the interferograms processed by the GRFN project using ISCE. Currently, the only mechanism available to filter the interferogram products using the Vertex interface is using the filenames. Use key terms like **S1-IFG_RM_M1S1_TN087**, **S1-IFG_RM_M1S2_TN087** and **S1-IFG_RM_M1S3_TN087**. The filenames appear to be related to number of Sentinel-1 SAFE modules that were stitched for the master / slave dates to generate the interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Filter Interferograms](notebook_images/filterIFGs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Step 3. Walk through search results and add interferogram-coherence products to the queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One can walk through the search result screen shots on the right and add the products of interest to the download queue. For this example, we only used the **Sentinel-1 Unwrapped Interferogram and Coherence Map (BETA)** products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Single Interferogram Product](notebook_images/singleIFG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Step 4. Check out the queue and download the python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Once you have added all the products that you want to the queue, select the **Download Queue** from the top-right section of the Vertex interface and download the python script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Download screen](notebook_images/Download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Continue exploring data ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### List downloaded products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ls download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looks like **download** subfolder contains each interferogram in its own zipfile with the master and slave dates as part of the file name. \n",
    "\n",
    "Data convention from each data source can be different. Follow best practices and always consult documentation for new data sources to confirm details. \n",
    "\n",
    "In this case, the dataset was processed using ISCE using the convention highlighted above. Let's look at contents of each interferogram folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### List contents of a single product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!zipinfo -1 download/S1-IFG_RM_M1S1_TN087_20180423T161551-20180330T161523_s1-resorb-73de-v1.2.1-standard.unw_geo.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each folder appears to include a filtered, unwrapped and geocoded interferogram named **filt_topophase.unw.geo** and a corresponding correlation file named **phsig.cor.geo**. These are standard ISCE products. We also corresponding **.vrt** files which will allow us to seamlessly import this data into GIS software. \n",
    "\n",
    "The next step is to understand the data layout of each of these products. We will use **gdal** for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Understand format of unwrapped, geocoded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!gdalinfo /vsizip/download/S1-IFG_RM_M1S1_TN087_20180423T161551-20180330T161523_s1-resorb-73de-v1.2.1-standard.unw_geo.zip/S1-IFG_RM_M1S1_TN087_20180423T161551-20180330T161523_s1-resorb-73de-v1.2.1-standard/merged/filt_topophase.unw.geo.vrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The unwrapped files appear to be 2-band float32 images. We know that the first band of ISCE unwrapped product is the interferogram amplitude and the second band is the unwrapped phase in radians. Packaging of data can vary depending on processing software and the archive. Always consult documentation from each archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Understand format of coherence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!gdalinfo /vsizip/download/S1-IFG_RM_M1S1_TN087_20180423T161551-20180330T161523_s1-resorb-73de-v1.2.1-standard.unw_geo.zip/S1-IFG_RM_M1S1_TN087_20180423T161551-20180330T161523_s1-resorb-73de-v1.2.1-standard/merged/phsig.cor.geo.vrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The unwrapped files appear to be 2-band float32 images. We know that the first band of ISCE unwrapped product is the interferogram amplitude and the second band is the unwrapped phase in radians. Packaging of data can vary depending on processing software and the archive. Always consult documentation from each archive.\n",
    "\n",
    "You will also note that the rasters for the unwrapped phase and the coherence layers in each product are the same size. They may vary from product to product. To check the geospatial extent of each product, we will quickly create a KML file of their bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 2. Simplification of filenames\n",
    "\n",
    "But before that we are going to create simpler filenames to work with. We are going to create VRT files in the master**yyyymmdd**-slave**yyyymmdd** format in a new subfolder called \"VRT\". This intermediate processing step is not needed for many data sources. We choose to use this approach for two reasons:\n",
    "1. Will make it easier for us to follow the rest of the exercise.\n",
    "2. Will allow us to work with the data without having to modify the contents of the downloaded products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "##We will retain downloaded data in zip files for demonstration\n",
    "import zipfile\n",
    "import glob\n",
    "import os\n",
    "from osgeo import gdal\n",
    "\n",
    "#Create VRT folder if it doesn't exist\n",
    "if not os.path.isdir('VRT'):\n",
    "    os.makedirs('VRT')\n",
    "\n",
    "##We will now loop over the zip files and create appropriate unw and cor files \n",
    "for zz in glob.glob('download/*.zip'):\n",
    "    basename = os.path.splitext(os.path.splitext(os.path.basename(zz))[0])[0]\n",
    "    unwsrc =  '/vsizip/' + os.path.join(os.path.abspath(zz), basename, 'merged', 'filt_topophase.unw.geo.vrt')\n",
    "    corsrc =  '/vsizip/' + os.path.join(os.path.abspath(zz), basename, 'merged', 'phsig.cor.geo.vrt')\n",
    "\n",
    "    masterdate = basename.split('-')[1][-15:-7]\n",
    "    slavedate = basename.split('-')[2][:8]\n",
    "    unwdest = os.path.join('VRT', masterdate + '-' + slavedate + '.unw.vrt')\n",
    "    cordest = os.path.join('VRT', masterdate + '-' + slavedate + '.cor.vrt')\n",
    "    gdal.Translate(unwdest, unwsrc, format='VRT', bandList=[2], noData=0.)\n",
    "    gdal.Translate(cordest, corsrc, format='VRT', bandList=[1], noData=0.)\n",
    "\n",
    "###Let's create one amplitude layer for visualization\n",
    "###We will just use the last unwrapped file for simplicity\n",
    "ampfile = unwdest.replace('unw', 'amp')\n",
    "gdal.Translate(ampfile, unwsrc, bandList=[1], noData=0.)\n",
    "\n",
    "###This is a unix command to build index from list of files\n",
    "!gdaltindex -f KML ProductBoundaries.kml VRT/*.unw.vrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Open **ProductBoundaries.kml** in Google Earth/QGIS to visualize the spatial extent of each product. Alternately, you can load the KML in python and visualize it as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###Plotting Product Boundaries on a map\n",
    "from osgeo import ogr\n",
    "\n",
    "driver = ogr.GetDriverByName('KML')  #'ESRI Shapefile' for shapefile\n",
    "kml_ds = driver.Open('ProductBoundaries.kml')\n",
    "polys = []\n",
    "for kml_lyr in kml_ds:\n",
    "    for feat in kml_lyr:\n",
    "        geom = feat.GetGeometryRef() #Polygon\n",
    "        ring = geom.GetGeometryRef(0)  #Ring corresponding to polygon\n",
    "        poly = np.array([list(x) for x in ring.GetPoints()])        #Points\n",
    "        polys.append(poly)\n",
    "\n",
    "kml_ds = None\n",
    "                     \n",
    "maxLat = np.max([np.max(x[:,1]) for x in polys])\n",
    "minLat = np.min([np.min(x[:,1]) for x in polys])              \n",
    "maxLon = np.max([np.max(x[:,0]) for x in polys])\n",
    "minLon = np.min([np.min(x[:,0]) for x in polys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "               \n",
    "\n",
    "mmap = Basemap(projection='tmerc', resolution='l',\n",
    "              llcrnrlon = minLon - 0.5,\n",
    "              llcrnrlat = minLat - 0.5,\n",
    "              urcrnrlon = maxLon + 0.5,\n",
    "              urcrnrlat = maxLat + 0.5,\n",
    "              lat_0 = 0.5 * (maxLat + minLat),\n",
    "              lon_0 = 0.5 * (maxLon + minLon ))\n",
    "mmap.drawmapboundary(fill_color='aqua')\n",
    "mmap.fillcontinents(color='green', lake_color='aqua')\n",
    "mmap.drawcoastlines()\n",
    "\n",
    "###Draw each polygon\n",
    "for poly in polys:\n",
    "    nativex, nativey = mmap(poly[:,0], poly[:,1])\n",
    "    nativepoly = Polygon( list(zip(nativex, nativey)), edgecolor='red')\n",
    "    plt.gca().add_patch(nativepoly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also create a quick view KMZ from the amplitude layer of an interferogram for visualization and to look at the coverage of the radar swath within the geocoded product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#!gdal_translate -of KMLSUPEROVERLAY -scale -exponent 0.3 -outsize 20% 20% {ampfile} amplitude.kmz -co FORMAT=JPEG\n",
    "gdal.Translate('amplitude.kmz', ampfile, \n",
    "               format='KMLSUPEROVERLAY', exponents=[0.3],\n",
    "               scaleParams=[''],\n",
    "               creationOptions=['FORMAT=JPEG'],\n",
    "               widthPct=20,\n",
    "               heightPct=20)\n",
    "\n",
    "###Matplotlib visualization\n",
    "###Create downlooked file on the fly\n",
    "ampds = gdal.Translate('', ampfile, \n",
    "               format='MEM',\n",
    "               widthPct=20,\n",
    "               heightPct=20)\n",
    "amp = ampds.ReadAsArray()\n",
    "geoTrans = ampds.GetGeoTransform()\n",
    "ampds = None\n",
    "minLon = geoTrans[0]\n",
    "maxLon = minLon + amp.shape[1] * geoTrans[1]\n",
    "maxLat = geoTrans[3]\n",
    "minLat = maxLat + amp.shape[0] * geoTrans[5]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(amp**0.1,\n",
    "           extent=[minLon, maxLon, minLat, maxLat],\n",
    "           cmap='gray')\n",
    "#plt.xlim([-155.25, -154.75])\n",
    "#plt.ylim([19.0, 19.4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are now ready to set up a script that will prepare this dataset for use within GIAnT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting up a python script for preparing data - prepGIAnT.py\n",
    "\n",
    "The recommended way to start working with data from a new processor or new source is to create an adapter script called **prepGIAnT.py**. This script is supposed to create input files and input python scripts, which will be then used by GIAnT. \n",
    "\n",
    "We will walk through various components of a single script, which we will run stage-by-stage. We will also use python comments within code sections to highlight changes that users can or should make to suit their work. This script is currently setup to work with products generated by GRFN and COMET. One observation is that these projects do not include extensive metadata with their products. We will specifically identify these locations. In future, when metadata is made available by these projects - the scripts should be updated to read in appropriate fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Import standard python libraries to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import datetime           ##To work with date-time representations\n",
    "import glob               ##To automatically search directories\n",
    "from osgeo import gdal    ##To read in geolocation metadata\n",
    "import os                 ##To create and remove directories\n",
    "import matplotlib.pyplot as plt  ##Quick plotting\n",
    "import numpy              ##Matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Setup input and output directory names\n",
    "\n",
    "We have already observed that the input data for this example is already aligned. Nevertheless, we will walk through the process of aligning the different datasets - by subsetting the region covered by the products. Hence, we will setup an input directory named **aligned_insar** which will contain the aligned products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insarDir  =  './VRT'              ##Directory that contains our interferograms\n",
    "alignedDir = './aligned_insar'      ##Directory where aligned subsets will be stored\n",
    "giantDir = './GIAnT'                ##Directory where we will perform the GIAnT analysis\n",
    "\n",
    "##Check if output directories already exist. If not create them\n",
    "if not os.path.isdir(alignedDir):\n",
    "    os.makedirs(alignedDir)\n",
    "    \n",
    "if not os.path.isdir(giantDir):\n",
    "    os.makedirs(giantDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Setup region of interest and reference region\n",
    "\n",
    "For this example, we will process a subset of the data and will use a region of 5 pixels x 5 pixels around a specified point as a reference region for time-series analysis. For subsetting the data, we use **amplitude.kmz** file that is generated above and pick an area around the rift zone.\n",
    "\n",
    "The final estimated time-series should be interpreted as displacement w.r.t the specified reference region. The reference region is ideally coherent in the interferograms and is assumed to have zero to slow deformation. Local site information is typically needed to select a good reference region.s\n",
    "\n",
    "The products used in the tutorial are geocode to a Lat/Lon grid\n",
    "If a UTM / Polar Stereographic grid is used - the regionOfInterest should be specified in the same coordinate system. Alternately, the script should include a section that converts coordinates from one system to another. *pyproj* or *gdal* itself can be used for coordinate transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionOfInterest = [19.25, 19.7, -155.25, -154.75]   ##SNWE convention from ISCE\n",
    "refRegionSize    = [5,5]                             ##Reference region size in Lines x Pixels\n",
    "refRegionCenter  = [19.5457, -155.044]  ###Point in south flank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Utilities to align data and to convert map coordinates to line, pixel\n",
    "\n",
    "We will now quickly define couple of simple functions which we will use in a loop to map align or subset all the input interferograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringToDate(instr):\n",
    "    '''\n",
    "    This is a simple function that will allow us to convert yyyymmdd to a python datetime object\n",
    "    '''\n",
    "    return datetime.datetime.strptime(instr, '%Y%m%d')\n",
    "\n",
    "def latLonToLinePixel(infile, latlon):\n",
    "    '''\n",
    "    This is a simple function that will read in lat, lon coordinates and return corresponding line, pixel\n",
    "    coordinates in a geocoded file\n",
    "    '''\n",
    "    \n",
    "    ds = gdal.Open(infile, gdal.GA_ReadOnly)  ##Open geocoded file for reading\n",
    "    trans = ds.GetGeoTransform()              ##Read in the geocoding information\n",
    "    ds = None                                 ##Closes the file\n",
    "    ##This is an excerpt from GDAL Documentation (http://www.gdal.org/gdal_tutorial.html)\n",
    "    ##For Geocoded images that are oriented North Top and West Left\n",
    "    ##adfGeoTransform[0] /* top left x */\n",
    "    ##adfGeoTransform[1] /* w-e pixel resolution */ \n",
    "    ##adfGeoTransform[2] /* 0 */\n",
    "    ##adfGeoTransform[3] /* top left y */\n",
    "    ##adfGeoTransform[4] /* 0 */\n",
    "    ##adfGeoTransform[5] /* n-s pixel resolution (negative value) */\n",
    "    \n",
    "    line = int(np.round((latlon[0] - trans[3])/trans[5]))\n",
    "    pixel = int(np.round((latlon[1] - trans[0])/trans[1]))\n",
    "    \n",
    "    return (line,pixel)\n",
    "\n",
    "\n",
    "def alignImageToRegion(infile, outfile, snwe, virtual=True, band=1):\n",
    "    '''\n",
    "    Align an input image to a specified region (snwe).\n",
    "    If virtual is False, output is in VRT format which is a simple XML and uses no extra disk space.\n",
    "    If virtual is True, output is in ENVI binary format which creates new files with subsets.\n",
    "    Additional options for regridding data, if products are on different grid sizes can also be accommodated.\n",
    "    '''\n",
    "    \n",
    "    ##Pick output format based on virtual flag\n",
    "    if virtual:\n",
    "        fmt = 'VRT'\n",
    "    else:\n",
    "        fmt = 'ENVI'\n",
    "    \n",
    "    ##We are essentially going to run\n",
    "    # gdal_translate -of fmt -b band -projwin w n e s infile outfile \n",
    "    # We are going to do this programmatically instead of making a system call\n",
    "    \n",
    "    #Set up options for gdal_translate command\n",
    "    opts = gdal.TranslateOptions(format=fmt, bandList=[band],\n",
    "                                 projWin=[snwe[2], snwe[1], snwe[3], snwe[0]])\n",
    "    #Open input file\n",
    "    ds = gdal.Open(infile, gdal.GA_ReadOnly)\n",
    "    \n",
    "    #Translate\n",
    "    gdal.Translate(outfile, ds, options=opts)\n",
    "    \n",
    "    #Close input file\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Looping over interferograms and gathering relevant data\n",
    "\n",
    "We have a very specific type of product for this tutorial. This section will need to be modified for each new source or processor. Always read the documentation to gather the necessary information to make these changes. For our specific case - we can make the following assumptions:\n",
    "\n",
    "1. Each interferogram subfolder contains one unwrapped file always named **filt_topophase.unw.geo.vrt** and one coherence file named **topophase.cor.geo**.\n",
    "\n",
    "2. We have unique interferograms - i.e, for a given pair of dates there is only one interferogram. Note that this is particularly an issue when a processing system can generate the same pair with different software version numbers. Based on the documentation from the data source, appropriate changes will need to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##First list all subfolders of insarDir\n",
    "ifgDirs = glob.glob( insarDir + '/*unw.vrt')\n",
    "print('Number of identified interferogram folders: {0}'.format(len(ifgDirs)))\n",
    "\n",
    "###Variable to store interferogram information\n",
    "ifgList = []\n",
    "dateList = []\n",
    "\n",
    "#Loop over interferograms\n",
    "for ifg in ifgDirs:\n",
    "    subdirName = os.path.basename(ifg)\n",
    "    master, slave = subdirName.split('.')[0].split('-')\n",
    "    \n",
    "    ###Start gathering data about the pair\n",
    "    data = {'masterDate' : master,                 ##Master date string\n",
    "            'slaveDate'  : slave,                  ##Slave date string\n",
    "            'folder'     : os.path.abspath( os.path.dirname(ifg))} ##Absolute path of folder- could even be URL/S3/zip file\n",
    "            \n",
    "    ###At this point, one could load baseline information and sensor information from the product folder\n",
    "    ###Currently, both GRFN and COMET don't include this information\n",
    "    ###For our example, we will also assume that this is not available and set it to constants\n",
    "    data['sensor'] = 'S1'\n",
    "    data['bperp'] = 0.0\n",
    "    data['btemp'] = ((stringToDate(master) - stringToDate(slave)).days)\n",
    "    \n",
    "    ##Few more fields can be calculated here if necessary - e.g, doppler centroid difference\n",
    "    \n",
    "    ##At this stage, we can also add a check to include only certain interferograms for analysis\n",
    "    ##Say Bperp range, Btemp range, Doppler range, with certain period etc\n",
    "    ##For now we will assume that we are using all interferograms\n",
    "    \n",
    "    if True:   ##True to be replaced with conditionals\n",
    "        ###Add master and slave to datelist\n",
    "        dateList.append(master)\n",
    "        dateList.append(slave)\n",
    "        \n",
    "        ifgList.append(data)   ##ifgList will be a list of dictionaries\n",
    "    \n",
    "###Determine unique dates\n",
    "dateList = sorted(list(set(dateList)))\n",
    "print('Number of unique SAR scenes: {0}'.format(len(dateList)))\n",
    "print('Number of interferograms retained: {0}'.format(len(ifgList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Some quick summary stats and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Quickly ensure that the network is connected\n",
    "Jmat = np.zeros((len(ifgList), len(dateList)))\n",
    "for ind, ifg in enumerate(ifgList):\n",
    "    Jmat[ind, dateList.index( ifg['masterDate'])] = 1\n",
    "    Jmat[ind, dateList.index( ifg['slaveDate'])] = -1\n",
    "    \n",
    "print('Number of connected components in interferogram network: {0}'.format(len(dateList) - numpy.linalg.matrix_rank(Jmat)))\n",
    "\n",
    "\n",
    "##Sort ifg list by dates. In our case, we have latest date as master consistently\n",
    "ifgList = sorted(ifgList, key = lambda x: x['slaveDate'])\n",
    "\n",
    "###Make a quick network coverage plot\n",
    "plt.figure('Coverage plot', figsize=(10,6))\n",
    "\n",
    "for ind, ifg in enumerate(ifgList):\n",
    "    plt.plot([stringToDate(ifg['masterDate']), stringToDate(ifg['slaveDate'])], [ind+1, ind+1])\n",
    "\n",
    "plt.xlabel('Acquisition time')\n",
    "plt.ylabel('Interferogram number')\n",
    "plt.show()   ###Can be replaced with plt.savefig('coverage.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g. Aligning or subsetting imagery\n",
    "\n",
    "In this section we will loop over the interferograms and generate aligned image files. For this tutorial, to save disk space we are going to use virtual files. You can read more about such files here: http://www.gdal.org/gdal_vrttut.html\n",
    "\n",
    "We assume the following for this tutorial:\n",
    "\n",
    "1. We will create aligned products in the **aligned_insar** subfolder with the following name scheme - **masteryyyymmdd_slaveyyyymmdd_unw.vrt** for unwrapped phase and ****masteryyyymmdd_slaveyyyymmdd_cor.vrt** for coherence.\n",
    "\n",
    "2. All the products have the same grid spacing of 1-arc sec. If not, the **alignImageToRegion** function should be modified to take in parameters for common grid spacing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ifg in ifgList:     ##For each interferogram\n",
    "    \n",
    "    ##Align unwrapped phase\n",
    "    alignImageToRegion( os.path.join(ifg['folder'], ifg['masterDate']+'-'+ifg['slaveDate']+'.unw.vrt'),\n",
    "                        os.path.join(alignedDir, ifg['masterDate']+'_'+ifg['slaveDate']+'_unw.vrt'),\n",
    "                        regionOfInterest, band=1)\n",
    "    \n",
    "    ##Align coherence\n",
    "    alignImageToRegion( os.path.join(ifg['folder'], ifg['masterDate']+'-'+ifg['slaveDate']+'.cor.vrt'),\n",
    "                        os.path.join(alignedDir, ifg['masterDate']+'_'+ifg['slaveDate']+'_cor.vrt'),\n",
    "                        regionOfInterest, band=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h. Preparing the interferogram network file - ifg.list\n",
    "\n",
    "GIAnT uses a simple 4 column text file to communicate the network information. This file will be created within the **GIAnT** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( os.path.join('GIAnT', 'ifg.list'), 'w') as fid:\n",
    "    for ifg in ifgList:\n",
    "        fid.write('{masterDate}  {slaveDate}  {bperp}  {sensor}\\n'.format(**ifg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Preparing a ROI_PAC rsc file - example.rsc\n",
    "\n",
    "GIAnT was first developed with ROI_PAC software. The easiest interface to providing some useful metadata to GIAnT is using an rsc file. Here we will build a bare minimum rsc file to use with GIAnT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##We will use one of the aligned output files to get dimensions\n",
    "exampleAlignedFile = os.path.join(alignedDir, ifgList[0]['masterDate']+'_'+ifgList[0]['slaveDate']+'_unw.vrt')\n",
    "ds = gdal.Open( exampleAlignedFile, gdal.GA_ReadOnly)\n",
    "nLines = ds.RasterYSize\n",
    "nPixels = ds.RasterXSize\n",
    "\n",
    "print(nLines, nPixels, 'SIZE')\n",
    "trans = ds.GetGeoTransform()\n",
    "ds = None\n",
    "\n",
    "\n",
    "with open(os.path.join(giantDir, 'example.rsc'), 'w') as fid:\n",
    "    fid.write('WIDTH     {0}\\n'.format(nPixels))\n",
    "    fid.write('FILE_LENGTH {0}\\n'.format(nLines))\n",
    "    \n",
    "    ##These fields can be obtained from metadata in each product when available\n",
    "    fid.write('HEADING_DEG  -12.0 \\n')    \n",
    "    fid.write('WAVELENGTH   0.031228381041666666\\n')\n",
    "    fid.write('CENTER_LINE_UTC   43200\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j. Preparing userfn.py to map interferogram dates to files on disk\n",
    "\n",
    "GIAnT uses a simple user defined function to map dates of SAR acquisitions to interferogram product names on disk. This mechanism allows one to reuse most of the input files and process different versions of the same stack with simple changes to this user defined function.\n",
    "\n",
    "**userfn.py** takes master date, slave date and sensor name to lookup a database or construct the filenames of the unwrapped phase and coherence products. In our case, these would be the names of the aligned products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relDir = os.path.relpath(alignedDir, giantDir)\n",
    "userfnTemplate = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "import os \n",
    "\n",
    "def makefnames(dates1, dates2, sensor):\n",
    "    dirname = '{0}'\n",
    "    root = os.path.join(dirname, dates1+'_'+dates2)\n",
    "    unwname = root+'_unw.vrt'\n",
    "    corname = root+'_cor.vrt'\n",
    "    return unwname, corname\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(giantDir, 'userfn.py'), 'w') as fid:\n",
    "    fid.write(userfnTemplate.format(relDir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k. Preparing a water mask (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not always needed for GIAnT. But in our case, we are working with data very close to the coastline and masking out data will significantly speed up the processing. This will also help out with visualization of the time-series outputs.\n",
    "\n",
    "There are several ways to generate the land water mask using different software (e.g, GMT, GDAL). For our tutorial, we downloaded a high resolution coastline shapefile from the Hawaii State GIS portal: \n",
    "http://geoportal.hawaii.gov/datasets/coastline?page=2\n",
    "\n",
    "We downloaded the coastline shapefile to the **coastline** folder. We will rasterize this shapefile using GDAL.\n",
    "```bash\n",
    "\n",
    "wget -O coastline/Coastline.zip  https://opendata.arcgis.com/datasets/045b1d5147634e2380566668e04094c6_3.zip\n",
    "```\n",
    "Note that we explicitly call our mask layer - water mask in this example. In reality, you can mask out any parts of your analysis area by providing an appropriate mask file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls coastline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zipinfo -1 coastline/Coastline.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ogrinfo /vsizip/coastline/Coastline.zip/Coastline.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Get the extent and spacing to rasterize to using an example aligned raster\n",
    "exampleAlignedFile = os.path.join(alignedDir, ifgList[0]['masterDate']+'_'+ifgList[0]['slaveDate']+'_unw.vrt')\n",
    "ds = gdal.Open( exampleAlignedFile, gdal.GA_ReadOnly)\n",
    "\n",
    "###Read in the coastline file\n",
    "shapeds = ogr.Open('/vsizip/coastline/Coastline.zip')\n",
    "layer = shapeds.GetLayerByIndex(0)\n",
    "\n",
    "##Create output Raster of desired size\n",
    "driver = gdal.GetDriverByName('MEM)\n",
    "maskds = driver.Create( '',\n",
    "                        ds.RasterXSize,\n",
    "                        ds.RasterYSize,\n",
    "                        1, gdal.GDT_Byte)\n",
    "maskds.SetProjection(ds.GetProjectionRef())\n",
    "maskds.SetGeoTransform(ds.GetGeoTransform())\n",
    "band = maskds.GetRasterBand(1)\n",
    "band.Fill(0)\n",
    "\n",
    "gdal.RasterizeLayer(maskds,  # output to our new dataset\n",
    "                    [1],  # output to our new dataset's first band\n",
    "                    layer,  # rasterize this layer\n",
    "                    None, None,  # don't worry about transformations since we're in same projection\n",
    "                    [1])  # burn value 1\n",
    "\n",
    "img = maskds.ReadAsArray()\n",
    "geoTrans = maskds.GetGeoTransform()\n",
    "\n",
    "ds = None\n",
    "shapeds = None\n",
    "maskds = None\n",
    "\n",
    "###Write water mask to GIAnT Directory\n",
    "img.astype(np.float32).tofile(os.path.join(giantDir, 'watermask.flt'))                            \n",
    "minLon = geoTrans[0]\n",
    "maxLon = minLon + img.shape[1] * geoTrans[1]\n",
    "maxLat = geoTrans[3]\n",
    "minLat = maxLat + img.shape[0] * geoTrans[5]\n",
    "plt.figure()\n",
    "plt.imshow(img, extent=[minLon, maxLon, minLat, maxLat], cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l. Preparing prepdataxml.py\n",
    "\n",
    "**prepdataxml.py** is a user defined function that is used to communicate some data pre-processing parameters and format information to GIAnT. The method is extensively documented in the user manual. For this tutorial, we will perform a simple setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepdataTemplate = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ######Prepare the data.xml\n",
    "    g = ts.TSXML('data')\n",
    "    g.prepare_data_xml('example.rsc', proc='RPAC',\n",
    "                       xlim=[0,{0}], ylim=[0, {1}],        ##Assuming no cropping\n",
    "                       rxlim = [{2},{3}], rylim=[{4},{5}], ##Reference region \n",
    "                       latfile='', lonfile='', hgtfile='', ##Needed only for tropospheric corrections\n",
    "                       inc = 21., cohth=0.3, chgendian='False',  ## Coherence thresholds\n",
    "                       unwfmt='GRD', corfmt='GRD',         ##Read data using GDAL\n",
    "                       mask='watermask.flt')         ##Only needed if mask file is used\n",
    "    g.writexml('data.xml')\n",
    "\"\"\"\n",
    "\n",
    "refLine, refPixel = latLonToLinePixel(exampleAlignedFile, refRegionCenter)\n",
    "\n",
    "with open( os.path.join(giantDir, 'prepdataxml.py'), 'w') as fid:\n",
    "    fid.write(prepdataTemplate.format(nPixels, nLines,\n",
    "                                      refPixel-refRegionSize[1]//2, refPixel+refRegionSize[1]//2, \n",
    "                                      refLine-refRegionSize[0]//2, refLine+refRegionSize[0]//2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m. Preparing prepsbasxml.py\n",
    "\n",
    "**prepsbasxml.py** is a user defined function that is used to communicate some SBAS-related processing to GIAnT. The method is extensively documented in the user manual. For this tutorial, we will perform a simple setup and use conventional SBAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepsbasTemplate  = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    g = ts.TSXML('params')\n",
    "    g. prepare_sbas_xml(nvalid = {0}, \n",
    "                        netramp=False,   ##No deramping requested\n",
    "                        atmos='',        ##No troposphere correction requested\n",
    "                        demerr = False,  ##No dem error correction requested\n",
    "                        uwcheck=False,   ##For future, not implemented yet\n",
    "                        regu=True,       ##For Mints. Not relevant for tutorial\n",
    "                        filt = 0.05)     ##Filter length in years\n",
    "    g.writexml('sbas.xml')\n",
    "\"\"\"\n",
    "\n",
    "with open( os.path.join(giantDir, 'prepsbasxml.py'), 'w') as fid:\n",
    "    fid.write(prepsbasTemplate.format( int(0.6*len(ifgList))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n. Confirm that the setup worked\n",
    "\n",
    "You should now be ready to use GIAnT to process this dataset. You should see the following when you run\n",
    "\n",
    "```\n",
    "> tree GIAnT\n",
    "GIAnT\n",
    "├── example.rsc\n",
    "├── ifg.list\n",
    "├── prepdataxml.py\n",
    "├── prepsbasxml.py\n",
    "├── watermask.flt\n",
    "└── userfn.py\n",
    "\n",
    "0 directories, 5 files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. Run SBAS analysis using GIAnT\n",
    "\n",
    "1. Change into GIAnT processing directory\n",
    "```\n",
    "> cd GIAnT\n",
    "```\n",
    "\n",
    "2. Prepare dataxml\n",
    "```\n",
    "> python prepdataxml.py\n",
    "```\n",
    "\n",
    "3. Gather all data and reference interferograms \n",
    "```\n",
    "> PrepIgramStack.py\n",
    "```\n",
    "\n",
    "4. Setup SBAS parameters\n",
    "```\n",
    ">python prepsbasxml.py\n",
    "```\n",
    "\n",
    "5. Run SBAS analysis\n",
    "```\n",
    "> SBASInvert.py\n",
    "```\n",
    "\n",
    "6. Run N-SBAS analysis\n",
    "```\n",
    "> NSBASInvert.py\n",
    "```\n",
    "\n",
    "7. Visualize output with plotts.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
